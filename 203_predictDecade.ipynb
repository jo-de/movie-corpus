{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"df_lines_per_decade.pkl\", \"rb\") as file:\n",
    "    df_lines_per_decade = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  meta.release_decade                                               text\n",
       "0                 192  -- but don't you remember?  i'm already dead. ...\n",
       "1                 193  yeah - sure! yes - sure if she can't pay, i'l...\n",
       "2                 194  -- he's just a lucky guy. monsieur rick, i -- ...\n",
       "3                 195  and his 'egghead' son!  we'll give 'em a <u>ro...\n",
       "4                 196  we're trying to get there. i hope we can. cont...\n",
       "5                 197  not at all, sir.  i have a pair of good pistol...\n",
       "6                 198  great, just great. that we do. and we put air ...\n",
       "7                 199  they do not! they do to! i hope so. she okay? ...\n",
       "8                 200  officers, there's your killer, do your duty, a...\n",
       "9                 201  no!!  i can prove it to you.  i'll take you to..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>meta.release_decade</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>192</td>\n      <td>-- but don't you remember?  i'm already dead. ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>193</td>\n      <td>yeah - sure! yes - sure if she can't pay, i'l...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>194</td>\n      <td>-- he's just a lucky guy. monsieur rick, i -- ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>195</td>\n      <td>and his 'egghead' son!  we'll give 'em a &lt;u&gt;ro...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>196</td>\n      <td>we're trying to get there. i hope we can. cont...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>197</td>\n      <td>not at all, sir.  i have a pair of good pistol...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>198</td>\n      <td>great, just great. that we do. and we put air ...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>199</td>\n      <td>they do not! they do to! i hope so. she okay? ...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>200</td>\n      <td>officers, there's your killer, do your duty, a...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>201</td>\n      <td>no!!  i can prove it to you.  i'll take you to...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "df_lines_per_decade\n",
    "# concatenate text\n",
    "df_lines_per_decade.groupby('meta.release_decade')['text'].apply(' '.join).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  meta.release_decade                                               text\n",
       "0                 192  -- but don't you remember?  i'm already dead. ...\n",
       "1                 193  yeah - sure! yes - sure if she can't pay, i'l...\n",
       "2                 194  -- he's just a lucky guy. monsieur rick, i -- ...\n",
       "3                 195  and his 'egghead' son!  we'll give 'em a <u>ro...\n",
       "4                 196  we're trying to get there. i hope we can. cont...\n",
       "5                 197  not at all, sir.  i have a pair of good pistol...\n",
       "6                 198  great, just great. that we do. and we put air ...\n",
       "7                 199  they do not! they do to! i hope so. she okay? ...\n",
       "8                 200  officers, there's your killer, do your duty, a...\n",
       "9                 201  no!!  i can prove it to you.  i'll take you to..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>meta.release_decade</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>192</td>\n      <td>-- but don't you remember?  i'm already dead. ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>193</td>\n      <td>yeah - sure! yes - sure if she can't pay, i'l...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>194</td>\n      <td>-- he's just a lucky guy. monsieur rick, i -- ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>195</td>\n      <td>and his 'egghead' son!  we'll give 'em a &lt;u&gt;ro...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>196</td>\n      <td>we're trying to get there. i hope we can. cont...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>197</td>\n      <td>not at all, sir.  i have a pair of good pistol...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>198</td>\n      <td>great, just great. that we do. and we put air ...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>199</td>\n      <td>they do not! they do to! i hope so. she okay? ...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>200</td>\n      <td>officers, there's your killer, do your duty, a...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>201</td>\n      <td>no!!  i can prove it to you.  i'll take you to...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "source": [
    "# find None lines\n",
    "df_lines_per_decade[df_lines_per_decade['text'].isnull()]\n",
    "\n",
    "# drop None lines which would cause errors\n",
    "df_lines_per_decade = df_lines_per_decade[df_lines_per_decade['text'].notnull()]\n",
    "\n",
    "# concatenate text\n",
    "df_lines_per_decade.groupby('meta.release_decade')['text'].apply(' '.join).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lines_per_decade = df_lines_per_decade[df_lines_per_decade['meta.release_decade'].isin(['193','194','195','196','197','198','199','200'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df_lines_per_decade, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "text sample: um... harry?  did i ever keep the books here?\nrelease_decade: 200\nTraining Data Shape: (203385, 7)\nTesting Data Shape: (100176, 7)\n"
     ]
    }
   ],
   "source": [
    "print('text sample:', train['text'].iloc[0])\n",
    "print('release_decade:', train['meta.release_decade'].iloc[0])\n",
    "print('Training Data Shape:', train.shape)\n",
    "print('Testing Data Shape:', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import string\n",
    "import re\n",
    "import spacy\n",
    "spacy.load('en')\n",
    "from spacy.lang.en import English\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "my_stop_words = \"well i'm\"\n",
    "STOPLIST = set(stopwords.words('english') + list(ENGLISH_STOP_WORDS) + my_stop_words.split()) \n",
    "SYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-\", \"...\", \"”\", \"”\", \"--\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanTextTransformer(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        return [cleanText(text) for text in X]\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "    \n",
    "def cleanText(text):\n",
    "    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def tokenizeText(sample):\n",
    "    tokens = parser(sample)\n",
    "    lemmas = []\n",
    "    for tok in tokens:\n",
    "        lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_)\n",
    "    tokens = lemmas\n",
    "    tokens = [tok for tok in tokens if tok not in STOPLIST]\n",
    "    tokens = [tok for tok in tokens if tok not in SYMBOLS]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "accuracy: 0.4680961507746366\nTop 10 features used to predict: \nClass 1 best: \n(-1.7834419219227404, 'musta')\n(-1.3919580095526651, 'wade')\n(-1.216202029583079, 'commissioner')\n(-1.2043261637862812, 'rick')\n(-1.1737497409992987, 'bein')\n(-1.1611666443731221, 'ed')\n(-1.1406432553760812, 'dalai')\n(-1.1333215366132756, 'fortress')\n(-1.126606302715779, 'venezuela')\n(-1.1105454941127495, 'johnny')\nClass 2 best: \n(1.979610617824478, 'conway')\n(1.930361604287997, 'dickson')\n(1.9078225481139643, 'ninotchka')\n(1.8909517459200749, 'studsy')\n(1.876728927496195, 'kringelein')\n(1.8752636304946455, 'wynant')\n(1.8574730878414625, 'anything\\x97')\n(1.8316175188791801, 'schuyler')\n(1.8268551908662585, 'preysing')\n(1.825137314736557, 'swana')\n"
     ]
    }
   ],
   "source": [
    "def printNMostInformative(vectorizer, clf, N):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    topClass1 = coefs_with_fns[:N]\n",
    "    topClass2 = coefs_with_fns[:-(N + 1):-1]\n",
    "    print(\"Class 1 best: \")\n",
    "    for feat in topClass1:\n",
    "        print(feat)\n",
    "    print(\"Class 2 best: \")\n",
    "    for feat in topClass2:\n",
    "        print(feat)\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=tokenizeText, ngram_range=(1,1))\n",
    "clf = LinearSVC()\n",
    "pipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer), ('clf', clf)])\n",
    "\n",
    "# data\n",
    "train1 = train['text'].tolist()\n",
    "labelsTrain1 = train['meta.release_decade'].tolist()\n",
    "\n",
    "test1 = test['text'].tolist()\n",
    "labelsTest1 = test['meta.release_decade'].tolist()\n",
    "# train\n",
    "pipe.fit(train1, labelsTrain1)\n",
    "\n",
    "# test\n",
    "preds = pipe.predict(test1)\n",
    "print(\"accuracy:\", accuracy_score(labelsTest1, preds))\n",
    "print(\"Top 10 features used to predict: \")\n",
    "\n",
    "printNMostInformative(vectorizer, clf, 10)\n",
    "\n",
    "pipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer)])\n",
    "transform = pipe.fit_transform(train1, labelsTrain1)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "\n",
    "for i in range(len(train1)):\n",
    "    s = \"\"\n",
    "    indexIntoVocab = transform.indices[transform.indptr[i]:transform.indptr[i+1]]\n",
    "    numOccurences = transform.data[transform.indptr[i]:transform.indptr[i+1]]\n",
    "    for idx, num in zip(indexIntoVocab, numOccurences):\n",
    "        s += str((vocab[idx], num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Number of classes, 8, does not match size of target_names, 10. Try specifying the labels parameter",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-6eab2c283af2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m print(metrics.classification_report(labelsTest1, preds, \n\u001b[0;32m----> 3\u001b[0;31m                                     target_names=df_lines_per_decade['meta.release_decade'].unique()))\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   1951\u001b[0m                 \u001b[0;34m\"Number of classes, {0}, does not match size of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1952\u001b[0m                 \u001b[0;34m\"target_names, {1}. Try specifying the labels \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1953\u001b[0;31m                 \u001b[0;34m\"parameter\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1954\u001b[0m             )\n\u001b[1;32m   1955\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Number of classes, 8, does not match size of target_names, 10. Try specifying the labels parameter"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(labelsTest1, preds, \n",
    "                                    target_names=df_lines_per_decade['meta.release_decade'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}